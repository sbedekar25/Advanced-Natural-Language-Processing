{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hidden Markov Models\n",
    "\n",
    "One application of HMMs is cryptoanalysis. Imagine a simple substitution code, in which\n",
    "every character of the English alphabet is substituted by another character, thus Banana\n",
    "could be translated into Ucpcpc. In order to decode the encrypted word, a bigram model\n",
    "could be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.) Assuming you do not know the code beforehand -- i.e., your task is to decrypt a message -- describe what the HMM would look like, in order to decode such words. How many and which states will the HMM have, and what do the emissions look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM has two parts\n",
    "1) A set of  states and a set of initial probabilities\n",
    "\n",
    "2) For bigram there  will be conditional probability functions for transitioning from one state to another.\n",
    "\n",
    "3) Also there will be probabilities to estimate  that a particular letter (A- Z) is associated with a particular cipher letter.\n",
    "\n",
    "4) The transition matrix will be 26 *26 assuming small letters\n",
    "So we have to normalize the text corpora\n",
    "\n",
    "Each state corresponds to one letter in the word banana (#states =6, states are 'b' ,'a','n','a','n','a')\n",
    "\n",
    "Emissions will be letters  \"U\",\"c\",\"p\".  \n",
    "\n",
    "We need to estimate  the probabbility that at any time t a letter in the set (A - Z) is associated with the hidden state (Cipher letters).\n",
    "\n",
    "Note this bigram so a hidden state has  a conditional probability based on what was the previous state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) Make a suggestion for how the emission probabilities should be distributed for a well-trained HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a well trained HMM the emission probabilities will be non zero. \n",
    "Also the emission Probabilities should be independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.) Would it make sense to convert the HMM to a trigram model instead of a bigram model? What are the advantages, what are the disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not make sense to use a Trigram model\n",
    "Disadvantages\n",
    "Require more letter word. For eg \"to\", \"at\", is will not be modelled correctly by a trigram model even if consider spaces\n",
    "\n",
    "Advantages\n",
    "Can capture  more patterns than a Bigram Model\n",
    "\n",
    "Cat at the door \n",
    "In a bigram model \"at in cat\" and \"at\" will be treated as same and will be counted towards the probability of letter t occuring given that a has occured\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Forward/Backward Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_vector = np.array([0.45, 0.35, 0.15, 0.05])\n",
    "\n",
    "# transition matrix          DT     JJ   NN   VB\n",
    "transition_matrix =np.array([[0.03,0.42,0.50,0.05], #DT\n",
    "                            [0.01,0.25,0.65,0.09],  #JJ\n",
    "                            [0.07,0.03,0.15,0.75],  #NN\n",
    "                            [0.30,0.25,0.15,0.30]]) #VB\n",
    "\n",
    "#emission probabilitities\n",
    "#                           DT   JJ   NN   VB\n",
    "emission_matrix =np.array([[0.84,0.05,0.03,0.05], #a\n",
    "                          [0.01,0.10,0.45,0.10],  #myth\n",
    "                          [0.02,0.02,0.02,0.60],  #is\n",
    "                          [0.84,0.05,0.03,0.05],  #a\n",
    "                          [0.01,0.70,0.25,0.05],  #femmale\n",
    "                          [0.12,0.13,0.25,0.20]]) #moth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"a\",\"myth\",\"is\",\"a\",\"female\",\"moth\"]\n",
    "pos_tags = ['DT', 'JJ', 'NN', 'VB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of forward probabilities\n",
    "forward_probabilities = np.zeros((4, len(words)))\n",
    "for w in range(len(words)):\n",
    "    for pos in range(len(pos_tags)):\n",
    "        emission_probability = emission_matrix[w, pos]\n",
    "        prevStateSum = 0\n",
    "        if w == 0:\n",
    "            prevStateSum = initial_vector[pos]\n",
    "        else:\n",
    "            for p_pos in range(len(pos_tags)):\n",
    "                transition_probability = transition_matrix[p_pos, pos]\n",
    "                prevStateSum = prevStateSum + transition_probability * forward_probabilities[p_pos, w - 1]\n",
    "        forward_probabilities[pos, w] = prevStateSum * emission_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.78000000e-01, 1.25800000e-04, 1.45011130e-04, 1.06482195e-02,\n",
       "        5.33952993e-06, 1.78656581e-05],\n",
       "       [1.75000000e-02, 1.63895000e-02, 1.49688970e-04, 5.32158902e-04,\n",
       "        3.34182452e-03, 1.16053595e-04],\n",
       "       [4.50000000e-03, 9.06412500e-02, 4.93625250e-04, 1.96856178e-04,\n",
       "        1.44931098e-03, 5.99794373e-04],\n",
       "       [2.50000000e-03, 2.46000000e-03, 4.21201695e-02, 6.51349618e-04,\n",
       "        4.61676149e-05, 2.80372941e-04]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of backward probabilities\n",
    "backward_probabilities = np.zeros((4, len(words)))\n",
    "for w in range(len(words) - 1, -1, -1):\n",
    "    for pos in range(len(pos_tags)):\n",
    "        emission_probability = emission_matrix[w, pos]\n",
    "        prevStateSum = 0\n",
    "        if w == len(words) - 1:\n",
    "            prevStateSum = 1\n",
    "        else:\n",
    "            for p_pos in range(len(pos_tags)):\n",
    "                transition_probability = transition_matrix[pos, p_pos]\n",
    "                prevStateSum = prevStateSum + transition_probability * backward_probabilities[p_pos, w + 1]\n",
    "        backward_probabilities[pos, w] = prevStateSum * emission_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11199151e-03, 8.11942904e-06, 8.21746124e-05, 7.42751184e-02,\n",
       "        1.93200000e-03, 1.20000000e-01],\n",
       "       [1.61891116e-04, 1.39966112e-04, 4.40158343e-05, 3.53594100e-03,\n",
       "        1.49940000e-01, 1.30000000e-01],\n",
       "       [3.18800348e-05, 4.86677460e-03, 1.43793640e-04, 5.50528200e-04,\n",
       "        4.99500000e-02, 2.50000000e-01],\n",
       "       [4.49298632e-05, 4.37179056e-04, 1.43818839e-02, 2.40235500e-03,\n",
       "        8.30000000e-03, 2.00000000e-01]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "α4(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00019685617823999994"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_probabilities[2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "α3(VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04212016949999999"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_probabilities[3,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "α1(DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.378"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_probabilities[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β4(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005505282000000001"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_probabilities[2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β2(NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00486677460100194"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_probabilities[2, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
